# Weather Data Pipeline With Apache Airflow ğŸŒ¤ï¸
Apache Airflow is a platform designed to manage and automate complex workflows through programmatic scheduling and monitoring. It empowers users to define workflows as Directed Acyclic Graphs (DAGs), which represent a series of tasks and their dependencies. Each DAG outlines the sequence in which tasks are executed, ensuring that they run in the correct order and adhere to specified dependencies.

### Table of Contents ğŸ“–
- [Project Overview](#project-overview) ğŸ”
- [Structure](#structure) ğŸ“‚
- [Installation](#installation) ğŸ“¦
- [Conclusion](#conclusion) ğŸ’¡

### Project Overview
This project showcases the use of Apache Airflow to orchestrate an ETL (Extract, Transform, Load) pipeline focused on weather data. It provides a practical example of how Airflow can be employed to automate the retrieval of weather information from the OpenWeatherMap API, process and transform the data, and present it in a structured format.

### Structure
```
â”Œâ”€â”€ api                         <-- API Folder
|   â””â”€â”€ api_key.py              <-- API Key Configuration
|
â”œâ”€â”€ dags                        <-- DAGs Folder
|   â””â”€â”€ *.py                    <-- DAGs Files
|
â”œâ”€â”€ data                        <-- Data Folder
|   â””â”€â”€ *.csv                   <-- Data Files
|
â”œâ”€â”€ logs                        <-- Log Folder
|   â””â”€â”€ *.log                   <-- Log Files
|
â”œâ”€â”€ src                         <-- Source Folder
|   â””â”€â”€ *.py                    <-- Source Files
|
â”œâ”€â”€ .env                        <-- Environment Variables
|
â”œâ”€â”€ .gitignore                  <-- Git Ignore Configuration
|
â”œâ”€â”€ .pre-commit-config.yaml     <-- Pre-Commit Configuration
|
â”œâ”€â”€ docker-compose.yaml         <-- Docker Compose Configuration
|
â”œâ”€â”€ readme.md                   <-- You Are Here
|
â””â”€â”€ requirements.txt            <-- Package Requirements
```

### Installation
To set up this project, follow these steps:

1. **Install Docker and Docker Compose:**
   - Follow the official [Docker installation guide](https://docs.docker.com/get-docker/) and [Docker Compose installation guide](https://docs.docker.com/compose/install/).
   - You can also watch [Install Apache Airflow](https://www.youtube.com/watch?v=Fl64Y0p7rls).

2. **Clone the Repository:**
   - Clone this repository to your local machine:
     ```bash
     git clone https://github.com/Aketirani/airflow-weather.git
     cd airflow-weather
     ```

3. **Configure API Key:**
   - Obtain an API key from [OpenWeatherMap](https://openweathermap.org/api).
   - Create a folder named `api` in the project root.
   - Inside the `api` folder, create a file named `api_key.py` with the following content:
     ```python
     API_KEY = "your_openweathermap_api_key_here"
     ```

4. **Start the Airflow Cluster:**
   - Run the following commands to start the Airflow services using Docker Compose:
     ```bash
     docker-compose build
     docker-compose up
     ```

### Conclusion
This project provides a foundational example of using Apache Airflow for workflow orchestration. It demonstrates basic DAG creation and task management, serving as a starting point for more complex workflows.
